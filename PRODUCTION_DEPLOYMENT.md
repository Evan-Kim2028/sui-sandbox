# Production Deployment Guide

## ğŸ¯ Current Status

**âœ… Complete Infrastructure:**
- Docker API with 7 endpoints (`/info`, `/validate`, `/schema`, `/health`, `/metrics`, `/tasks/{id}/results`, `/`)
- Prometheus metrics for monitoring
- Webhook callbacks for async workflows
- Partial results API for progress tracking
- Full config validation (26 fields)
- 166 passing tests

**âœ… Testing Tools Ready:**
- `scripts/test_multi_model_integration.py` - Multi-model testing
- `scripts/query_benchmark_logs.py` - Results analysis & cost tracking
- Complete documentation (`docs/INTEGRATION_TESTING.md`, `TESTING_QUICKSTART.md`)

**âš ï¸ Corpus Required:**
- Docker container needs a properly structured corpus at `/app/corpus`
- Corpus structure: `/app/corpus/{package_id}/bytecode_modules/*.mv`
- Manifest file: `/app/corpus/manifest.txt` with one package ID per line

---

## ğŸš€ Quick Start (When You Have a Corpus)

### 1. Prepare Corpus

```bash
# Your corpus should look like:
corpus/
â”œâ”€â”€ manifest.txt
â”œâ”€â”€ 0xc681beced336875c26f1410ee5549138425301b08725ee38e625544b9eaaade7/
â”‚   â””â”€â”€ bytecode_modules/
â”‚       â”œâ”€â”€ module1.mv
â”‚       â””â”€â”€ module2.mv
â”œâ”€â”€ 0x2df868f30120484cc5e900c3b8b7a04561596cf15a9751159a207930471afff2/
â”‚   â””â”€â”€ bytecode_modules/
â”‚       â””â”€â”€ *.mv
...
```

### 2. Mount Corpus in Docker

Edit `docker-compose.yml`:

```yaml
volumes:
  - /path/to/your/corpus:/app/corpus:ro
  - ./benchmark/results:/app/results
  - ./benchmark/logs:/app/logs
```

### 3. Start Services

```bash
cd /path/to/sui-move-interface-extractor

# Build and start
docker compose up -d --wait smi-bench

# Verify running
curl http://localhost:9999/info | jq .
```

### 4. Run Tests

**Option A: Real-World Test Script (NEW - Recommended)**

Fast, flexible multi-model testing with detailed analytics:

```bash
cd benchmark

# Quick dry-run test (free, ~2s per package)
uv run python3 run_real_world_test.py \
    --samples 2 \
    --models gpt-4o-mini,google/gemini-3-flash-preview \
    --simulation-mode dry-run

# Live production test (uses model APIs, ~30-120s per package)
uv run python3 run_real_world_test.py \
    --samples 5 \
    --models gpt-4o,google/gemini-3-flash-preview,google/gemini-2.5-flash-preview \
    --simulation-mode live
```

**Key Features:**
- âœ… Test any combination of models
- âœ… Mix fast and slow models (e.g., GPT-4o-mini + GPT-4o)
- âœ… Dry-run mode (mock agent, no API costs)
- âœ… Live mode (real model APIs, cost tracking)
- âœ… Detailed analytics (duration, tokens, errors, hit rate)
- âœ… Results saved to `real_world_test_results_*.json`

**Documentation:** [QUICK_START_GUIDE.md](QUICK_START_GUIDE.md)

---

**Option B: Multi-Model Integration Script**

Comprehensive testing with multiple scenarios:

```bash
cd benchmark

# Multi-model integration test
./scripts/test_multi_model_integration.py \
  --corpus-root /app/corpus \
  --manifest /app/corpus/manifest.txt
```

**Documentation:** [benchmark/TESTING_QUICKSTART.md](benchmark/TESTING_QUICKSTART.md)

---

**Query Results**

```bash
cd benchmark

# List all runs
./scripts/query_benchmark_logs.py list

# Filter by model
./scripts/query_benchmark_logs.py list --model gpt-4o

# Show run details
./scripts/query_benchmark_logs.py show <RUN_ID>

# Compare runs
./scripts/query_benchmark_logs.py compare <RUN_ID_1> <RUN_ID_2>

# Calculate costs
./scripts/query_benchmark_logs.py cost <RUN_ID>
```

---

## ğŸ“Š What Gets Tracked

### Per Run
- **Model**: `gpt-4o-mini`, `gpt-4o`, `claude-3-5-sonnet`, etc.
- **Duration**: Total time + per-package breakdown
- **Tokens**: Prompt & completion (for cost calculation)
- **Hit Rate**: Success metric (created types / target types)
- **Errors**: Package failures, timeouts

### Storage

**Logs**: `logs/{run_id}/`
- `run_metadata.json` - Config, timing, model info
- `events.jsonl` - Streaming event log

**Results**: `results/a2a/{run_id}.json`
- Aggregate metrics
- Per-package detailed results
- Token counts, cost estimates

---

## ğŸ” Monitoring & Analysis

### Real-Time Monitoring (Prometheus)

```bash
# View metrics
curl http://localhost:9999/metrics

# Key metrics:
# - smi_bench_task_duration_seconds (task performance)
# - smi_bench_task_requests_total (throughput)
# - smi_bench_active_tasks (current load)
# - smi_bench_task_errors_total (failure rate)
```

**Grafana Setup:**
1. Add Prometheus datasource pointing to `:9999/metrics`
2. Create dashboards for:
   - Task duration by model
   - Success rate trends
   - Cost per run
   - Package-level performance

### Historical Analysis

```bash
# List all runs
./scripts/query_benchmark_logs.py list

# Filter by model
./scripts/query_benchmark_logs.py list --model gpt-4o

# Detailed analysis
./scripts/query_benchmark_logs.py analyze <RUN_ID>

# Cost calculation
./scripts/query_benchmark_logs.py cost <RUN_ID>

# Compare models
./scripts/query_benchmark_logs.py compare <RUN_ID_1> <RUN_ID_2>
```

---

## ğŸ’° Cost Tracking

Token usage automatically tracked with every run (real agents only).

**Current Pricing** (update in `scripts/query_benchmark_logs.py`):
- GPT-4o-mini: $0.15/$0.60 per 1M tokens (input/output)
- GPT-4o: $2.50/$10.00 per 1M tokens
- Claude-3.5-Sonnet: $3.00/$15.00 per 1M tokens

**Estimate Costs:**
```bash
# Single run
./scripts/query_benchmark_logs.py cost a2a_phase2_1234567890

# All runs from today
for run in results/a2a/*.json; do
  RUN_ID=$(basename $run .json)
  ./scripts/query_benchmark_logs.py cost $RUN_ID
done | jq -s 'map(.total_cost_usd) | add'
```

---

## ğŸ§ª Testing Workflows

### 1. Quick Smoke Test (Mock Agent)

```bash
# Fast validation (no LLM calls)
curl -X POST http://localhost:9999 \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": "1",
    "method": "message/send",
    "params": {
      "message": {
        "messageId": "msg_smoke",
        "role": "user",
        "parts": [{
          "text": "{\"config\": {\"corpus_root\": \"/app/corpus\", \"package_ids_file\": \"/app/corpus/manifest.txt\", \"samples\": 1, \"agent\": \"mock-empty\"}}"
        }]
      }
    }
  }'
```

### 2. Model Comparison

Edit `scripts/test_multi_model_integration.py`:

```python
# Test 3 models on same packages
await tester.test_scenario("gpt4o_mini", model="gpt-4o-mini", samples=2)
await tester.test_scenario("gpt4o", model="gpt-4o", samples=2)
await tester.test_scenario("claude", model="claude-3-5-sonnet-20241022", samples=2)
```

Run:
```bash
./scripts/test_multi_model_integration.py --corpus-root /app/corpus --manifest /app/corpus/manifest.txt
```

### 3. Webhook Workflow

```bash
# Submit with callback
curl -X POST http://localhost:9999 \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc": "2.0",
    "id": "1",
    "method": "message/send",
    "params": {
      "message": {
        "messageId": "msg_webhook",
        "role": "user",
        "parts": [{
          "text": "{\"config\": {\"corpus_root\": \"/app/corpus\", \"package_ids_file\": \"/app/corpus/manifest.txt\", \"callback_url\": \"https://your-service.com/webhook\"}}"
        }]
      }
    }
  }'

# Results will be POST'd to your webhook when complete
```

---

## ğŸ› Troubleshooting

### Container Won't Start

```bash
# Check logs
docker compose logs smi-bench

# Rebuild
docker compose build smi-bench
docker compose up -d --wait smi-bench
```

### Task Stuck "Running"

**Possible causes:**
1. **Missing corpus** - Packages in manifest don't exist in `/app/corpus`
2. **Wrong corpus structure** - Needs `{package_id}/bytecode_modules/*.mv`
3. **API key missing** - Check SMI_MODEL env var for real agents

**Debug:**
```bash
# Check latest logs
ls -lt benchmark/logs/ | head -5

# View events
cat benchmark/logs/<run_id>/events.jsonl

# Check container filesystem
docker exec smi-bench-dev ls /app/corpus
docker exec smi-bench-dev cat /app/corpus/manifest.txt
```

### No Token Counts

Token tracking requires `agent: real-openai-compatible`. Mock agents don't track tokens.

### Wrong Cost Estimates

Update pricing in `scripts/query_benchmark_logs.py`:
```python
pricing = {
    "input_per_1k": <your_rate> / 1000,
    "output_per_1k": <your_rate> / 1000,
}
```

---

## ğŸ“ File Locations

```
.
â”œâ”€â”€ docker-compose.yml           # Service config
â”œâ”€â”€ Dockerfile                   # Container image
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ test_multi_model_integration.py  # Testing script
â”‚   â”‚   â””â”€â”€ query_benchmark_logs.py          # Analysis tool
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ INTEGRATION_TESTING.md   # Full testing guide
â”‚   â”‚   â””â”€â”€ A2A_EXAMPLES.md          # API examples
â”‚   â”œâ”€â”€ TESTING_QUICKSTART.md        # Quick reference
â”‚   â”œâ”€â”€ logs/                        # Run logs
â”‚   â”‚   â””â”€â”€ {run_id}/
â”‚   â”‚       â”œâ”€â”€ run_metadata.json
â”‚   â”‚       â””â”€â”€ events.jsonl
â”‚   â””â”€â”€ results/a2a/                 # Final results
â”‚       â””â”€â”€ {run_id}.json
â””â”€â”€ PRODUCTION_DEPLOYMENT.md         # This file
```

---

## âœ… Production Checklist

Before deploying to production:

- [ ] Corpus mounted and validated
- [ ] Manifest file points to existing packages
- [ ] API keys configured (SMI_MODEL env var)
- [ ] Prometheus scraping configured
- [ ] Grafana dashboards created
- [ ] Log retention policy set
- [ ] Cost alerts configured
- [ ] Webhook endpoint tested (if using)
- [ ] Integration tests passing
- [ ] Backup strategy for results/logs

---

## ğŸ“š Documentation

- **[Getting Started](benchmark/GETTING_STARTED.md)** - Benchmark introduction
- **[Testing Guide](benchmark/docs/INTEGRATION_TESTING.md)** - Complete testing reference
- **[API Examples](benchmark/docs/A2A_EXAMPLES.md)** - All endpoints with examples
- **[Quick Start](benchmark/TESTING_QUICKSTART.md)** - 5-minute testing guide
- **[Architecture](benchmark/docs/ARCHITECTURE.md)** - System design

---

## ğŸ¯ Next Steps

1. **Obtain or build corpus** - Download/extract Move packages
2. **Validate corpus structure** - Ensure bytecode_modules directories exist
3. **Run smoke test** - Verify Docker + API working
4. **Run model comparison** - Test 2-3 models on small subset
5. **Analyze results** - Use query tool to find optimal model
6. **Set up monitoring** - Connect Grafana to metrics endpoint
7. **Production deployment** - Scale up to full corpus
