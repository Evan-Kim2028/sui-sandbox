//! Generic package->replay context commands.
//!
//! Canonical command: `context` (alias: `flow`).

use anyhow::{anyhow, Context, Result};
use base64::Engine;
use clap::{ArgAction, Args, Parser, Subcommand, ValueEnum};
use move_core_types::account_address::AccountAddress;
use serde_json::json;
use std::path::PathBuf;

use super::fetch::fetch_package_into_state;
use super::replay::{FetchStrategy, ReplayCmd, ReplayHydrationArgs, ReplayProfile, ReplaySource};
use super::state::ObjectMetadata;
use super::tools::HistoricalSeriesCmd;
use super::SandboxState;
use sui_sandbox_core::checkpoint_discovery::{
    build_walrus_client as core_build_walrus_client,
    discover_checkpoint_targets as core_discover_checkpoint_targets,
    resolve_replay_target_from_discovery as core_resolve_replay_target_from_discovery,
    DiscoverOutput as CoreDiscoverOutput, WalrusArchiveNetwork as CoreWalrusArchiveNetwork,
};
use sui_sandbox_core::environment_bootstrap::{
    hydrate_build_and_finalize_mainnet_environment, EnvironmentBuildPlan, EnvironmentFinalizePlan,
    MainnetHydrationPlan, MainnetObjectRequest,
};
use sui_sandbox_core::utilities::collect_required_package_roots_from_type_strings;
use sui_transport::walrus::WalrusClient;

mod context_io;
use context_io::{
    default_flow_context_path, load_context_file_into_state, prepare_context_data,
    write_context_file,
};

#[derive(Parser, Debug)]
#[command(about = "Generic two-step package/replay context flow (flow alias)")]
pub struct FlowCli {
    #[command(subcommand)]
    command: FlowSubcommand,
}

#[derive(Subcommand, Debug)]
enum FlowSubcommand {
    /// Hydrate package/object state and initialize a replay-ready local environment
    Bootstrap(FlowBootstrapCmd),
    /// Prepare a reusable package context (fetch package + deps)
    Prepare(FlowPrepareCmd),
    /// Replay a transaction with optional prepared context
    Replay(FlowReplayCmd),
    /// Run prepare + replay in one command
    Run(FlowRunCmd),
    /// Discover replay-ready digests + packages from checkpoints
    Discover(FlowDiscoverCmd),
    /// Execute a historical view function across a checkpoint/version series
    HistoricalSeries(HistoricalSeriesCmd),
}

#[derive(Args, Debug)]
pub struct FlowBootstrapCmd {
    /// Root package ids to hydrate (repeat flag or pass comma-separated list)
    #[arg(long = "package-id", required = true, value_delimiter = ',')]
    pub package_ids: Vec<String>,

    /// Optional type references used to infer additional package roots
    #[arg(long = "type-ref")]
    pub type_refs: Vec<String>,

    /// Optional object IDs to hydrate at latest version
    #[arg(long = "object")]
    pub objects: Vec<String>,

    /// Optional object@version pins to hydrate historical object state
    #[arg(long = "object-at")]
    pub object_at: Vec<String>,

    /// Sender address for local environment initialization
    #[arg(
        long,
        default_value = "0x0000000000000000000000000000000000000000000000000000000000000000"
    )]
    pub sender: String,

    /// Historical-mode provider (strict versioned hydration paths)
    #[arg(long, default_value_t = false)]
    pub historical_mode: bool,

    /// When object@version is missing, allow latest-object fallback
    #[arg(long, default_value_t = true, action = ArgAction::Set)]
    pub allow_latest_object_fallback: bool,

    /// Fail bootstrap when object loading into the local environment fails
    #[arg(long, default_value_t = true, action = ArgAction::Set)]
    pub fail_on_object_load: bool,

    /// Parent object IDs whose dynamic-field wrappers should be preloaded
    #[arg(long = "dynamic-field-parent")]
    pub dynamic_field_parents: Vec<String>,

    /// Dynamic-field preload scan limit per parent
    #[arg(long, default_value_t = 32)]
    pub dynamic_field_limit: usize,

    /// Configure on-demand gRPC fetchers in the environment
    #[arg(long, default_value_t = true, action = ArgAction::Set)]
    pub configure_fetchers: bool,
}

#[derive(Args, Debug)]
pub struct FlowPrepareCmd {
    /// Root package id to prepare
    #[arg(long = "package-id")]
    pub package_id: String,

    /// Also fetch transitive dependencies (default: true)
    #[arg(long = "with-deps", default_value_t = true, action = ArgAction::Set)]
    pub with_deps: bool,

    /// Output context JSON path
    #[arg(long)]
    pub output: Option<PathBuf>,

    /// Overwrite existing context file
    #[arg(long, default_value_t = false)]
    pub force: bool,
}

#[derive(Args, Debug)]
pub struct FlowReplayCmd {
    /// Transaction digest to replay
    pub digest: Option<String>,

    /// Optional context file generated by `context prepare` (or `flow prepare`)
    #[arg(long)]
    pub context: Option<PathBuf>,

    /// Optional package id to fetch before replay (if --context is not provided)
    #[arg(long = "package-id")]
    pub package_id: Option<String>,

    /// Also fetch transitive dependencies when using --package-id (default: true)
    #[arg(long = "with-deps", default_value_t = true, action = ArgAction::Set)]
    pub with_deps: bool,

    #[command(flatten)]
    pub target: ReplayTargetArgs,

    #[command(flatten)]
    pub walrus: WalrusEndpointArgs,

    #[command(flatten)]
    pub execution: ReplayExecutionArgs,
}

#[derive(Args, Debug)]
pub struct FlowRunCmd {
    /// Root package id to prepare
    #[arg(long = "package-id")]
    pub package_id: String,

    /// Transaction digest to replay (optional when --state-json has a single state)
    #[arg(long)]
    pub digest: Option<String>,

    /// Also fetch transitive dependencies (default: true)
    #[arg(long = "with-deps", default_value_t = true, action = ArgAction::Set)]
    pub with_deps: bool,

    /// Optional path to persist context JSON (portable for later --context replay)
    #[arg(long = "context-out")]
    pub context_out: Option<PathBuf>,

    /// Overwrite existing context file when --context-out is used
    #[arg(long, default_value_t = false)]
    pub force: bool,

    #[command(flatten)]
    pub target: ReplayTargetArgs,

    #[command(flatten)]
    pub walrus: WalrusEndpointArgs,

    #[command(flatten)]
    pub execution: ReplayExecutionArgs,
}

#[derive(Args, Debug)]
pub struct FlowDiscoverCmd {
    /// Checkpoint spec: single (239615926), range (239615900..239615926), or list (239615900,239615910)
    #[arg(long, conflicts_with = "latest")]
    pub checkpoint: Option<String>,

    /// Scan latest N checkpoints (auto-discovers tip)
    #[arg(long, conflicts_with = "checkpoint")]
    pub latest: Option<u64>,

    /// Optional package filter (only include transactions touching this package)
    #[arg(long = "package-id")]
    pub package_id: Option<String>,

    /// Include framework packages (0x1/0x2/0x3) in results
    #[arg(long, default_value_t = false)]
    pub include_framework: bool,

    /// Max matching transactions to return
    #[arg(long, default_value_t = 200)]
    pub limit: usize,

    #[command(flatten)]
    pub walrus: WalrusEndpointArgs,
}

#[derive(Args, Debug, Clone, PartialEq, Eq)]
pub struct ReplayTargetArgs {
    /// Optional checkpoint override (recommended for walrus source)
    #[arg(long)]
    pub checkpoint: Option<u64>,

    /// Auto-discover a digest from latest N checkpoints for replay target selection
    #[arg(long, conflicts_with_all = ["digest", "state_json", "checkpoint"])]
    pub discover_latest: Option<u64>,

    /// Optional state JSON for deterministic custom replay input data
    #[arg(long = "state-json")]
    pub state_json: Option<PathBuf>,
}

#[derive(Args, Debug, Clone, PartialEq, Eq)]
pub struct WalrusEndpointArgs {
    /// Walrus archive network used for discovery
    #[arg(long, value_enum, default_value = "mainnet")]
    pub walrus_network: WalrusArchiveNetwork,

    /// Override Walrus caching endpoint (requires --walrus-aggregator-url)
    #[arg(long)]
    pub walrus_caching_url: Option<String>,

    /// Override Walrus aggregator endpoint (requires --walrus-caching-url)
    #[arg(long)]
    pub walrus_aggregator_url: Option<String>,
}

#[derive(Args, Debug, Clone, PartialEq, Eq)]
pub struct ReplayExecutionArgs {
    /// Replay hydration source
    #[arg(long, value_enum, default_value = "hybrid")]
    pub source: ReplaySource,

    /// Runtime defaults profile (tunes fallback and transport behavior)
    #[arg(long, value_enum, default_value = "balanced")]
    pub profile: ReplayProfile,

    /// Fetch strategy for dynamic field children during replay
    #[arg(long, value_enum, default_value = "full")]
    pub fetch_strategy: FetchStrategy,

    /// Allow fallback hydration paths when data is missing
    #[arg(long = "allow-fallback", default_value_t = true, action = ArgAction::Set)]
    pub allow_fallback: bool,

    /// Prefetch depth for dynamic fields
    #[arg(long, default_value_t = 3)]
    pub prefetch_depth: usize,

    /// Prefetch limit per dynamic-field parent
    #[arg(long, default_value_t = 200)]
    pub prefetch_limit: usize,

    /// Disable dynamic-field prefetch
    #[arg(long, default_value_t = false)]
    pub no_prefetch: bool,

    /// Auto-inject system objects (Clock/Random) when missing
    #[arg(long, default_value_t = true, action = ArgAction::Set)]
    pub auto_system_objects: bool,

    /// Compare local replay against on-chain effects
    #[arg(long, default_value_t = false)]
    pub compare: bool,

    /// Hydration-only mode (skip VM execution and print replay-state summary)
    #[arg(long, default_value_t = false)]
    pub analyze_only: bool,

    /// VM-only mode (disable fallback paths)
    #[arg(long, default_value_t = false)]
    pub vm_only: bool,

    /// Reconcile dynamic-field effects when on-chain lists omit them
    #[arg(long, default_value_t = true, action = ArgAction::Set)]
    pub reconcile_dynamic_fields: bool,

    /// Synthesize placeholder inputs when replay fails on missing objects
    #[arg(long, default_value_t = false)]
    pub synthesize_missing: bool,

    /// Allow dynamic-field reads to synthesize placeholder values when missing
    #[arg(long, default_value_t = false)]
    pub self_heal_dynamic_fields: bool,

    /// Timeout in seconds for gRPC object fetches
    #[arg(long, default_value_t = 30)]
    pub grpc_timeout_secs: u64,

    /// Local replay cache path (used when --source local)
    #[arg(long)]
    pub cache_dir: Option<PathBuf>,

    /// Fail command when replay output indicates mismatch/failure
    #[arg(long, default_value_t = false)]
    pub strict: bool,
}

#[derive(Debug, Clone, Copy, ValueEnum, PartialEq, Eq)]
pub enum WalrusArchiveNetwork {
    Mainnet,
    Testnet,
}

impl From<WalrusArchiveNetwork> for CoreWalrusArchiveNetwork {
    fn from(value: WalrusArchiveNetwork) -> Self {
        match value {
            WalrusArchiveNetwork::Mainnet => CoreWalrusArchiveNetwork::Mainnet,
            WalrusArchiveNetwork::Testnet => CoreWalrusArchiveNetwork::Testnet,
        }
    }
}

type FlowDiscoverOutput = CoreDiscoverOutput;

impl ReplayExecutionArgs {
    fn to_replay_cmd(
        &self,
        digest: Option<String>,
        checkpoint: Option<u64>,
        state_json: Option<PathBuf>,
    ) -> ReplayCmd {
        ReplayCmd {
            digest,
            hydration: ReplayHydrationArgs {
                source: self.source,
                cache_dir: self.cache_dir.clone(),
                allow_fallback: self.allow_fallback,
                prefetch_depth: self.prefetch_depth,
                prefetch_limit: self.prefetch_limit,
                no_prefetch: self.no_prefetch,
                auto_system_objects: self.auto_system_objects,
            },
            profile: self.profile,
            vm_only: self.vm_only,
            strict: self.strict,
            compare: self.compare,
            analyze_only: self.analyze_only,
            verbose: false,
            fetch_strategy: self.fetch_strategy,
            reconcile_dynamic_fields: self.reconcile_dynamic_fields,
            synthesize_missing: self.synthesize_missing,
            self_heal_dynamic_fields: self.self_heal_dynamic_fields,
            grpc_timeout_secs: self.grpc_timeout_secs,
            checkpoint: checkpoint.map(|value| value.to_string()),
            state_json,
            export_state: None,
            latest: None,
        }
    }
}

impl FlowCli {
    pub async fn execute(
        &self,
        state: &mut SandboxState,
        json_output: bool,
        verbose: bool,
    ) -> Result<()> {
        match &self.command {
            FlowSubcommand::Bootstrap(cmd) => cmd.execute(state, json_output, verbose).await,
            FlowSubcommand::Prepare(cmd) => cmd.execute(state, json_output, verbose).await,
            FlowSubcommand::Replay(cmd) => cmd.execute(state, json_output, verbose).await,
            FlowSubcommand::Run(cmd) => cmd.execute(state, json_output, verbose).await,
            FlowSubcommand::Discover(cmd) => cmd.execute(json_output).await,
            FlowSubcommand::HistoricalSeries(cmd) => cmd.execute(json_output).await,
        }
    }
}

impl FlowBootstrapCmd {
    async fn execute(
        &self,
        state: &mut SandboxState,
        json_output: bool,
        _verbose: bool,
    ) -> Result<()> {
        let explicit_roots: Vec<AccountAddress> = self
            .package_ids
            .iter()
            .map(|raw| {
                AccountAddress::from_hex_literal(raw)
                    .with_context(|| format!("invalid --package-id value {}", raw))
            })
            .collect::<Result<Vec<_>>>()?;
        let package_roots =
            collect_required_package_roots_from_type_strings(&explicit_roots, &self.type_refs)?
                .into_iter()
                .collect::<Vec<_>>();

        let mut object_requests = Vec::with_capacity(self.objects.len() + self.object_at.len());
        for object_id in &self.objects {
            validate_hex_address(object_id, "--object")?;
            object_requests.push(MainnetObjectRequest {
                object_id: object_id.clone(),
                version: None,
            });
        }
        for spec in &self.object_at {
            object_requests.push(parse_object_at_spec(spec)?);
        }

        let sender = AccountAddress::from_hex_literal(&self.sender)
            .with_context(|| format!("invalid --sender value {}", self.sender))?;
        let finalize_plan = EnvironmentFinalizePlan {
            dynamic_field_parents: self.dynamic_field_parents.clone(),
            dynamic_field_limit: self.dynamic_field_limit,
            configure_fetchers: self.configure_fetchers,
        };
        let bootstrapped = hydrate_build_and_finalize_mainnet_environment(
            &MainnetHydrationPlan {
                package_roots,
                objects: object_requests,
                historical_mode: self.historical_mode,
                allow_latest_object_fallback: self.allow_latest_object_fallback,
            },
            &EnvironmentBuildPlan {
                sender,
                fail_on_object_load: self.fail_on_object_load,
            },
            &finalize_plan,
        )
        .await?;

        let mut packages_loaded = 0usize;
        for (addr, package) in &bootstrapped.hydration.packages {
            state.add_package(*addr, package.modules.clone());
            packages_loaded += 1;
        }
        let mut objects_loaded = 0usize;
        let mut objects_skipped_missing_type_tag = 0usize;
        for (object_id, (bytes, type_tag, version, is_shared)) in &bootstrapped.hydration.objects {
            let Some(type_tag) = type_tag.as_deref() else {
                objects_skipped_missing_type_tag += 1;
                continue;
            };
            let b64 = base64::engine::general_purpose::STANDARD.encode(bytes);
            state.add_object_with_metadata(
                object_id,
                Some(type_tag),
                &b64,
                ObjectMetadata {
                    version: *version,
                    is_shared: *is_shared,
                    ..Default::default()
                },
            )?;
            objects_loaded += 1;
        }

        let registration = &bootstrapped.build.package_registration;
        if !registration.failed.is_empty() {
            let first = &registration.failed[0];
            return Err(anyhow!(
                "package registration failed for {} package(s); first failure: {} ({})",
                registration.failed.len(),
                first.0.to_hex_literal(),
                first.1
            ));
        }

        if json_output {
            println!(
                "{}",
                serde_json::to_string_pretty(&json!({
                    "success": true,
                    "provider_endpoint": bootstrapped.hydration.provider.grpc_endpoint(),
                    "historical_mode": self.historical_mode,
                    "packages_hydrated": bootstrapped.hydration.packages.len(),
                    "objects_hydrated": bootstrapped.hydration.objects.len(),
                    "packages_loaded_into_session": packages_loaded,
                    "objects_loaded_into_session": objects_loaded,
                    "objects_skipped_missing_type_tag": objects_skipped_missing_type_tag,
                    "environment": {
                        "sender": sender.to_hex_literal(),
                        "package_registration": {
                            "loaded": registration.loaded,
                            "skipped_upgraded": registration.skipped_upgraded,
                            "failed": registration.failed,
                        },
                        "objects_loaded": bootstrapped.build.objects_loaded,
                        "dynamic_fields_loaded": bootstrapped.finalize.dynamic_fields_loaded,
                        "fetchers_configured": bootstrapped.finalize.fetchers_configured,
                    },
                }))?
            );
        } else {
            println!("Bootstrap complete:");
            println!(
                "  provider endpoint: {}",
                bootstrapped.hydration.provider.grpc_endpoint()
            );
            println!(
                "  packages hydrated: {}",
                bootstrapped.hydration.packages.len()
            );
            println!(
                "  objects hydrated: {}",
                bootstrapped.hydration.objects.len()
            );
            println!(
                "  package registration: loaded={}, skipped_upgraded={}, failed={}",
                registration.loaded,
                registration.skipped_upgraded,
                registration.failed.len()
            );
            println!(
                "  runtime finalize: dynamic_fields_loaded={}, fetchers_configured={}",
                bootstrapped.finalize.dynamic_fields_loaded,
                bootstrapped.finalize.fetchers_configured
            );
            println!(
                "  session primed: packages={}, objects={} (skipped_missing_type_tag={})",
                packages_loaded, objects_loaded, objects_skipped_missing_type_tag
            );
        }

        Ok(())
    }
}

impl FlowPrepareCmd {
    pub(crate) async fn execute(
        &self,
        state: &mut SandboxState,
        json_output: bool,
        verbose: bool,
    ) -> Result<()> {
        let output_path = self
            .output
            .clone()
            .unwrap_or_else(|| default_flow_context_path(&self.package_id));

        let (context, packages_fetched) =
            prepare_context_data(state, &self.package_id, self.with_deps, verbose)?;
        write_context_file(&output_path, &context, self.force)?;

        if json_output {
            println!(
                "{}",
                serde_json::to_string_pretty(&json!({
                    "success": true,
                    "context_path": output_path,
                    "context_metadata": {
                        "version": context.version,
                        "package_id": context.package_id,
                        "with_deps": context.with_deps,
                        "rpc_url": context.rpc_url,
                        "generated_at_ms": context.generated_at_ms,
                    },
                    "packages_fetched_count": packages_fetched.len(),
                }))?
            );
        } else {
            println!("Flow context prepared:");
            println!("  package_id:   {}", self.package_id);
            println!("  with_deps:    {}", self.with_deps);
            println!("  packages:     {}", packages_fetched.len());
            println!("  context_path: {}", output_path.display());
            println!(
                "\nNext:\n  sui-sandbox context replay <DIGEST> --context {} --checkpoint <CP>",
                output_path.display()
            );
        }
        Ok(())
    }
}

impl FlowReplayCmd {
    pub(crate) async fn execute(
        &self,
        state: &mut SandboxState,
        json_output: bool,
        verbose: bool,
    ) -> Result<()> {
        if self.context.is_some() && self.package_id.is_some() {
            return Err(anyhow!(
                "Provide either --context or --package-id, not both"
            ));
        }

        let mut prepared_package: Option<(String, usize)> = None;
        if let Some(path) = self.context.as_ref() {
            let loaded = load_context_file_into_state(state, path, verbose)?;
            prepared_package = Some((loaded.package_id, loaded.packages_count));
        } else if let Some(package_id) = self.package_id.as_ref() {
            let fetched = fetch_package_into_state(state, package_id, self.with_deps, verbose)
                .with_context(|| format!("Failed to prefetch package {}", package_id))?;
            prepared_package = Some((package_id.clone(), fetched.packages_fetched.len()));
        }

        if !json_output {
            if let Some((pkg, count)) = prepared_package.as_ref() {
                println!("Prepared package context: {} ({} packages)", pkg, count);
            }
        }

        let (effective_digest, effective_checkpoint) = resolve_replay_target(
            self.digest.as_deref(),
            self.target.state_json.as_ref(),
            self.target.checkpoint,
            self.target.discover_latest,
            prepared_package.as_ref().map(|(pkg, _)| pkg.as_str()),
            self.walrus.walrus_network,
            self.walrus.walrus_caching_url.as_deref(),
            self.walrus.walrus_aggregator_url.as_deref(),
        )?;

        if !json_output {
            if let (Some(window), Some(digest), Some(checkpoint)) = (
                self.target.discover_latest,
                effective_digest.as_deref(),
                effective_checkpoint,
            ) {
                println!(
                    "Auto-discovered replay target: digest={} checkpoint={} (latest window: {})",
                    digest, checkpoint, window
                );
            }
        }

        let replay = self.execution.to_replay_cmd(
            effective_digest,
            effective_checkpoint,
            self.target.state_json.clone(),
        );

        replay.execute(state, json_output, verbose).await
    }
}

impl FlowRunCmd {
    pub(crate) async fn execute(
        &self,
        state: &mut SandboxState,
        json_output: bool,
        verbose: bool,
    ) -> Result<()> {
        let (effective_digest, effective_checkpoint) = resolve_replay_target(
            self.digest.as_deref(),
            self.target.state_json.as_ref(),
            self.target.checkpoint,
            self.target.discover_latest,
            Some(self.package_id.as_str()),
            self.walrus.walrus_network,
            self.walrus.walrus_caching_url.as_deref(),
            self.walrus.walrus_aggregator_url.as_deref(),
        )?;

        let (context, packages_fetched) =
            prepare_context_data(state, &self.package_id, self.with_deps, verbose)?;
        let context_path = if let Some(path) = self.context_out.as_ref() {
            write_context_file(path, &context, self.force)?;
            Some(path.display().to_string())
        } else {
            None
        };

        if !json_output {
            println!(
                "Prepared package context: {} ({} packages)",
                self.package_id,
                packages_fetched.len()
            );
            if let Some(path) = context_path.as_deref() {
                println!("Context written: {}", path);
            }
            if let (Some(window), Some(digest), Some(checkpoint)) = (
                self.target.discover_latest,
                effective_digest.as_deref(),
                effective_checkpoint,
            ) {
                println!(
                    "Auto-discovered replay target: digest={} checkpoint={} (latest window: {})",
                    digest, checkpoint, window
                );
            }
        }

        let replay = self.execution.to_replay_cmd(
            effective_digest,
            effective_checkpoint,
            self.target.state_json.clone(),
        );

        replay.execute(state, json_output, verbose).await
    }
}

impl FlowDiscoverCmd {
    pub(crate) async fn execute(&self, json_output: bool) -> Result<()> {
        let walrus = build_walrus_client(
            self.walrus.walrus_network,
            self.walrus.walrus_caching_url.as_deref(),
            self.walrus.walrus_aggregator_url.as_deref(),
        )?;
        let output = discover_flow_targets(
            &walrus,
            self.checkpoint.as_deref(),
            self.latest,
            self.package_id.as_deref(),
            self.include_framework,
            self.limit,
        )?;

        if json_output {
            println!("{}", serde_json::to_string_pretty(&output)?);
        } else {
            println!("Discovery summary:");
            println!("  checkpoints_scanned: {}", output.checkpoints_scanned);
            println!("  transactions_scanned: {}", output.transactions_scanned);
            println!("  ptbs_scanned: {}", output.ptbs_scanned);
            println!("  matches: {}", output.matches);
            if let Some(filter) = output.package_filter.as_deref() {
                println!("  package_filter: {}", filter);
            }
            if output.truncated {
                println!("  truncated: yes (increase --limit to see more)");
            }
            if output.targets.is_empty() {
                println!("No matching transactions found.");
            } else {
                println!("\nTargets:");
                for target in &output.targets {
                    println!(
                        "  cp={} digest={} sender={} packages={} calls={}",
                        target.checkpoint,
                        target.digest,
                        target.sender,
                        target.package_ids.join(","),
                        target.move_calls.len()
                    );
                }
            }
        }
        Ok(())
    }
}

#[allow(clippy::too_many_arguments)]
fn resolve_replay_target(
    digest: Option<&str>,
    state_json: Option<&PathBuf>,
    checkpoint: Option<u64>,
    discover_latest: Option<u64>,
    discover_package_id: Option<&str>,
    walrus_network: WalrusArchiveNetwork,
    walrus_caching_url: Option<&str>,
    walrus_aggregator_url: Option<&str>,
) -> Result<(Option<String>, Option<u64>)> {
    let walrus = build_walrus_client(walrus_network, walrus_caching_url, walrus_aggregator_url)?;
    core_resolve_replay_target_from_discovery(
        digest,
        checkpoint,
        state_json.is_some(),
        discover_latest,
        discover_package_id,
        &walrus,
    )
    .map_err(|err| {
        let message = err.to_string();
        if message == "digest cannot be empty" {
            anyhow!("--digest cannot be empty")
        } else if message
            == "provide digest, state_file, or discover_latest for replay target selection"
        {
            anyhow!("Provide --digest, --state-json, or --discover-latest for replay target selection")
        } else if message == "discover_package_id is required when discover_latest is set" {
            anyhow!(
                "--discover-latest requires package context; pass --package-id or --context with package_id"
            )
        } else {
            err
        }
    })
}

fn discover_flow_targets(
    walrus: &WalrusClient,
    checkpoint_spec: Option<&str>,
    latest: Option<u64>,
    package_id: Option<&str>,
    include_framework: bool,
    limit: usize,
) -> Result<FlowDiscoverOutput> {
    core_discover_checkpoint_targets(
        walrus,
        checkpoint_spec,
        latest,
        package_id,
        include_framework,
        limit,
    )
}

#[cfg(test)]
fn parse_checkpoint_spec(spec: &str) -> Result<Vec<u64>> {
    sui_sandbox_core::checkpoint_discovery::parse_checkpoint_spec(spec)
}

fn build_walrus_client(
    network: WalrusArchiveNetwork,
    caching_url: Option<&str>,
    aggregator_url: Option<&str>,
) -> Result<WalrusClient> {
    core_build_walrus_client(network.into(), caching_url, aggregator_url).map_err(|err| {
        let message = err.to_string();
        if message.contains("provide both walrus_caching_url and walrus_aggregator_url") {
            anyhow!("provide both --walrus-caching-url and --walrus-aggregator-url for custom endpoints")
        } else {
            err
        }
    })
}

fn parse_object_at_spec(spec: &str) -> Result<MainnetObjectRequest> {
    let trimmed = spec.trim();
    let (object_id, version_raw) = trimmed.rsplit_once('@').ok_or_else(|| {
        anyhow!(
            "invalid --object-at value `{}` (expected <OBJECT_ID>@<VERSION>)",
            spec
        )
    })?;
    validate_hex_address(object_id, "--object-at")?;
    let version: u64 = version_raw.trim().parse().with_context(|| {
        format!(
            "invalid version `{}` in --object-at value `{}`",
            version_raw, spec
        )
    })?;
    Ok(MainnetObjectRequest {
        object_id: object_id.trim().to_string(),
        version: Some(version),
    })
}

fn validate_hex_address(raw: &str, flag: &str) -> Result<()> {
    let trimmed = raw.trim();
    AccountAddress::from_hex_literal(trimmed)
        .with_context(|| format!("invalid {} value {}", flag, raw))?;
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::{build_walrus_client, parse_checkpoint_spec, FlowCli, WalrusArchiveNetwork};
    use super::{
        FetchStrategy, FlowReplayCmd, FlowRunCmd, ReplayExecutionArgs, ReplayProfile, ReplaySource,
        ReplayTargetArgs, WalrusEndpointArgs,
    };
    use clap::Parser;
    use serde_json::json;
    use std::path::PathBuf;
    use sui_sandbox_core::context_contract::parse_context_payload;

    #[test]
    fn parses_flow_prepare() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "prepare",
            "--package-id",
            "0x2",
            "--with-deps",
            "true",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_bootstrap() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "bootstrap",
            "--package-id",
            "0x2",
            "--type-ref",
            "0x2::sui::SUI",
            "--object",
            "0x6",
            "--object-at",
            "0x6@1",
            "--dynamic-field-parent",
            "0x6",
            "--dynamic-field-limit",
            "16",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_replay_with_context() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "replay",
            "At8M8D7QoW3HHXUBHHvrsdhko8hEDdLAeqkZBjNSKFk2",
            "--context",
            "/tmp/ctx.json",
            "--checkpoint",
            "239615926",
            "--source",
            "walrus",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_replay_with_discover_latest() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "replay",
            "--package-id",
            "0x2",
            "--discover-latest",
            "5",
            "--source",
            "walrus",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_run() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "run",
            "--package-id",
            "0x2",
            "--digest",
            "At8M8D7QoW3HHXUBHHvrsdhko8hEDdLAeqkZBjNSKFk2",
            "--checkpoint",
            "239615926",
            "--source",
            "walrus",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_run_with_discover_latest() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "run",
            "--package-id",
            "0x2",
            "--discover-latest",
            "5",
            "--source",
            "walrus",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_discover() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "discover",
            "--latest",
            "5",
            "--package-id",
            "0x2",
            "--limit",
            "50",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_discover_with_custom_walrus_overrides() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "discover",
            "--latest",
            "5",
            "--package-id",
            "0x2",
            "--walrus-network",
            "testnet",
            "--walrus-caching-url",
            "https://example-caching.invalid",
            "--walrus-aggregator-url",
            "https://example-aggregator.invalid",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn parses_flow_historical_series() {
        let parsed = FlowCli::try_parse_from([
            "flow",
            "historical-series",
            "--request-file",
            "examples/data/deepbook_margin_state/manager_state_request.json",
            "--series-file",
            "examples/data/deepbook_margin_state/position_b_daily_timeseries.json",
        ]);
        assert!(parsed.is_ok());
    }

    #[test]
    fn rejects_partial_custom_walrus_endpoint_pair() {
        let err = build_walrus_client(
            WalrusArchiveNetwork::Mainnet,
            Some("https://example-caching.invalid"),
            None,
        )
        .expect_err("missing aggregator should fail");
        assert!(err
            .to_string()
            .contains("provide both --walrus-caching-url"));
    }

    #[test]
    fn accepts_full_custom_walrus_endpoint_pair() {
        let client = build_walrus_client(
            WalrusArchiveNetwork::Mainnet,
            Some("https://example-caching.invalid"),
            Some("https://example-aggregator.invalid"),
        )
        .expect("custom pair should construct client");
        let _ = client;
    }

    #[test]
    fn parses_checkpoint_range_spec() {
        let checkpoints = parse_checkpoint_spec("239615920..239615923").expect("range parse");
        assert_eq!(
            checkpoints,
            vec![239615920, 239615921, 239615922, 239615923]
        );
    }

    #[test]
    fn parses_checkpoint_list_spec() {
        let checkpoints = parse_checkpoint_spec("5, 3,5,7").expect("list parse");
        assert_eq!(checkpoints, vec![3, 5, 7]);
    }

    #[test]
    fn rejects_inverted_checkpoint_range() {
        let err = parse_checkpoint_spec("20..10").expect_err("inverted range should fail");
        assert!(err.to_string().contains("end must be >= start"));
    }

    #[test]
    fn parses_object_at_spec() {
        let parsed = super::parse_object_at_spec("0x6@123").expect("object-at parse");
        assert_eq!(parsed.object_id, "0x6");
        assert_eq!(parsed.version, Some(123));
    }

    #[test]
    fn parses_python_v1_context_wrapper() {
        let value = json!({
            "version": 1,
            "package_id": "0x2",
            "resolve_deps": true,
            "generated_at_ms": 0,
            "packages": {
                "0x2": []
            }
        });
        let parsed = parse_context_payload(&value).expect("python context should parse");
        assert_eq!(parsed.package_id.as_deref(), Some("0x2"));
        assert!(parsed.with_deps);
        assert_eq!(parsed.packages.len(), 1);
        assert_eq!(parsed.packages[0].address, "0x2");
    }

    #[test]
    fn parses_python_v2_context_wrapper() {
        let value = json!({
            "version": 2,
            "package_id": "0x2",
            "with_deps": true,
            "generated_at_ms": 0,
            "packages": [
                {
                    "address": "0x2",
                    "modules": [],
                    "bytecodes": []
                }
            ]
        });
        let parsed = parse_context_payload(&value).expect("python v2 context should parse");
        assert_eq!(parsed.package_id.as_deref(), Some("0x2"));
        assert!(parsed.with_deps);
        assert_eq!(parsed.packages.len(), 1);
        assert_eq!(parsed.packages[0].address, "0x2");
    }

    #[test]
    fn parses_direct_package_map_context() {
        let value = json!({
            "0x2": []
        });
        let parsed = parse_context_payload(&value).expect("package map context should parse");
        assert_eq!(parsed.package_id, None);
        assert!(parsed.with_deps);
        assert_eq!(parsed.packages.len(), 1);
        assert_eq!(parsed.packages[0].address, "0x2");
    }

    #[test]
    fn rejects_non_context_object() {
        let value = json!({
            "version": 1
        });
        let err = parse_context_payload(&value).expect_err("invalid object should fail");
        assert!(err.to_string().contains("missing `packages` field"));
    }

    #[test]
    fn replay_and_run_map_identical_replay_options() {
        let replay = FlowReplayCmd {
            digest: Some("At8M8D7QoW3HHXUBHHvrsdhko8hEDdLAeqkZBjNSKFk2".to_string()),
            context: Some(PathBuf::from("/tmp/context.json")),
            package_id: None,
            with_deps: true,
            target: ReplayTargetArgs {
                checkpoint: Some(239_615_926),
                discover_latest: None,
                state_json: Some(PathBuf::from("/tmp/state.json")),
            },
            walrus: WalrusEndpointArgs {
                walrus_network: WalrusArchiveNetwork::Mainnet,
                walrus_caching_url: None,
                walrus_aggregator_url: None,
            },
            execution: ReplayExecutionArgs {
                source: ReplaySource::Walrus,
                profile: ReplayProfile::Fast,
                fetch_strategy: FetchStrategy::Eager,
                allow_fallback: false,
                prefetch_depth: 7,
                prefetch_limit: 123,
                no_prefetch: true,
                auto_system_objects: false,
                compare: true,
                analyze_only: true,
                vm_only: true,
                reconcile_dynamic_fields: false,
                synthesize_missing: true,
                self_heal_dynamic_fields: true,
                grpc_timeout_secs: 77,
                cache_dir: Some(PathBuf::from("/tmp/cache")),
                strict: true,
            },
        };
        let run = FlowRunCmd {
            package_id: "0x2".to_string(),
            digest: replay.digest.clone(),
            with_deps: replay.with_deps,
            context_out: None,
            force: false,
            target: replay.target.clone(),
            walrus: replay.walrus.clone(),
            execution: replay.execution.clone(),
        };
        assert_eq!(replay.target, run.target);
        assert_eq!(replay.walrus, run.walrus);
        assert_eq!(replay.execution, run.execution);
    }
}
